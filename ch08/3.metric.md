# 모니터링

분산 시스템에서는 장애가 “어디서” 시작됐는지 알기 어렵다.  
요청 하나가 여러 서비스, 여러 인프라 레이어를 거치기 때문이다.

이때 필요한 것이 **관측성(Observability)** 이다.  
관측성은 단순 모니터링이 아니라, **“내부 상태를 외부에서 이해할 수 있는 능력”** 이다.

관측성의 3요소:

- **로그(Log)** → “무슨 일이 있었는가?”
- **메트릭(Metric)** → “지금 상태가 어떤가?”
- **트레이싱(Trace)** → “어디서 느려졌고, 어디서 실패했는가?”

이 세 가지가 연결될 때 장애 분석이 빠르고 정확해진다.

---

## 8.5 분산 시스템 로깅

로그는 이벤트 중심 데이터다.  
에러, 상태 변화, 중요한 처리 흐름이 기록된다.

### 8.5.1 중앙 집중형 로깅

분산 환경에서는 로그가 서버마다 흩어져 있으면 의미가 없다.  
반드시 **중앙 수집 → 저장 → 검색 가능** 구조가 필요하다.

#### 중앙 집중형 로깅의 장점
- 장애 원인 분석 시간 단축(MTTR 감소)
- 서버 수가 늘어나도 운영 복잡도 증가를 억제
- 보안 감사, 사용자 행위 추적 가능

#### 대표 스택
- **ELK / EFK**
  - Elasticsearch: 저장/검색
  - Logstash/Fluentd/Fluent Bit: 수집
  - Kibana: 시각화
- **Loki + Grafana**
  - 라벨 기반 로그 시스템
  - 메트릭과 대시보드 연계가 쉬움

---

### 8.5.2 분산 로깅 모범 사례

#### 1) 구조화 로그(JSON)
텍스트 로그는 사람이 읽기엔 좋지만 기계 처리엔 불리하다.  
JSON 구조 로그는 검색, 집계, 필터링이 쉽다.

예 필드:
- timestamp
- level
- service
- message
- traceId
- userId

---

#### 2) 컨텍스트 정보 포함 (매우 중요)
분산 시스템에서는 **“이 로그가 어떤 요청의 일부인지”** 알아야 한다.

필수:
- requestId 또는 traceId
- 사용자 ID, 주문 ID 등 도메인 식별자

---

#### 3) 예외 스택 트레이스 기록
- 단순히 “에러 발생”이 아니라
  원인 체인(cause)과 스택 정보를 남겨야 분석 가능

---

#### 4) 로그 레벨 정책

| 레벨 | 용도 |
|---|---|
| DEBUG | 개발/문제 재현용 |
| INFO | 정상 흐름의 핵심 이벤트 |
| WARN | 비정상 상황이지만 서비스는 동작 |
| ERROR | 실패, 예외 |

로그 레벨을 남용하면:
- DEBUG 남발 → 저장 비용 폭증
- ERROR 과다 → 진짜 장애 신호 묻힘

---

#### 5) 로그 순환(Rotation) 및 보관 정책
- 디스크 고갈 방지
- 보안/감사 요구사항에 맞는 보관 기간 설정

---

## 8.6 메트릭

메트릭은 **수치 기반 상태 데이터**다.  
로그가 사건 기록이라면, 메트릭은 “현재 건강 상태”를 보여준다.

---

### 8.6.1 메트릭 종류

#### 1) 시스템 메트릭
- CPU 사용률
- 메모리 사용량
- 디스크 I/O
- 네트워크 트래픽

→ 인프라 레벨 문제 탐지

---

#### 2) 애플리케이션 메트릭
- QPS(Request per second)
- 응답 시간(latency)
- 오류율(error rate)
- 스레드 풀, GC

→ 서비스 품질(SLO) 판단 기준

---

#### 3) 비즈니스 메트릭
- 결제 성공률
- 가입 수
- 주문 건수

→ 시스템이 “비즈니스에 미치는 영향”을 보여줌

---

### 8.6.2 메트릭 도구

| 도구 | 특징 |
|---|---|
| Prometheus | Pull 기반, CNCF 표준, 쿠버네티스 친화적 |
| Graphite | 오래된 시계열 DB |
| Datadog | SaaS, 통합성 높음 |

---

### 8.6.3 메트릭 구현 모범 사례

#### 1) 핵심 메트릭 선정 (Golden Signals)
- Latency (지연 시간)
- Traffic (요청량)
- Errors (오류율)
- Saturation (자원 포화)

#### 2) 일관된 네이밍 규칙
예)
- `http_requests_total`
- `http_request_duration_seconds`

#### 3) 오류율 + 지연시간 중심 모니터링
대부분 장애는 “느려짐” 또는 “실패 증가”로 나타난다.

#### 4) 시각화(대시보드)
- 서비스 단위
- 인프라 단위
- 도메인 단위

---

## 8.7 알림

알림은 “문제가 발생했음을 알리는 시스템”이다.  
하지만 알림이 많아지면 오히려 중요한 신호를 놓친다.

---

### 8.7.1 알림 설계 원칙

#### 1) 중요도 구분
- Critical: 즉시 대응 필요
- Warning: 추적 필요
- Info: 참고

#### 2) 실행 가능성(Actionable)
알림을 받았을 때 “무엇을 해야 하는지”가 명확해야 한다.

#### 3) 충분한 맥락 정보
- 서비스 이름
- 메트릭 값과 임계치
- 관련 대시보드 링크

---

### 8.7.2 알림 도구
- Prometheus Alertmanager
- Grafana Alert

---

### 8.7.3 알림 모범 사례
- 알림 과다 방지(노이즈 제거)
- 알림 테스트
- 대응 절차 문서화(Runbook)

---

## 8.8 분산 트레이싱

트레이싱은 요청의 흐름을 시간 순으로 추적하는 기술이다.

---

### 8.8.1 개념

- **Trace**: 하나의 요청 전체 흐름
- **Span**: 그 요청의 한 구간(DB 호출, API 호출 등)

---

### 8.8.2 도구
- Jaeger
- Zipkin
- OpenTelemetry (표준 SDK)

---

### 8.8.3 구현 모범 사례

#### 1) 주요 구간에 Span 추가
- API 진입
- DB 쿼리
- 외부 서비스 호출

#### 2) 컨텍스트 전파
- traceId, spanId를 서비스 간 전달
- 로그에도 traceId 포함

#### 3) Span 이름 명확히
- `GET /orders/{id}`
- `DB SELECT orders`

---

### 8.8.4 왜 중요한가?
메트릭은 “느리다”를 알려주고,  
로그는 “에러가 났다”를 알려주지만,  
트레이싱은 **“어디서 느렸고, 어디서 실패했는지”** 를 보여준다.