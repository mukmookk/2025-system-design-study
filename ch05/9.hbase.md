## 목차

- [5.9 HBase](#59-hbase)
  - [5.9.1 HBase 데이터의 특징](#591-hbase-데이터의-특징)
  - [5.9.2 HBase의 특징 정리](#592-hbase의-특징-정리)
  - [5.9.1 HBase 자세히 살펴보기](#591-hbase-자세히-살펴보기)

## 5.9 HBase

### 5.9.1 HBase 데이터의 특징

- 구글 Bigtable 모델 기반 오픈소스 구현 (Hadoop 에코 시스템의 한 축)
- HDFS 위에 구축된 분산 컬럼 패밀리 스토리지
- Row Key 기반 정렬 + 범위 파티셔닝
- Column Family 단위의 물리적 저장
- 셀(Cell) 단위로 다중 버전(타임스탬프) 저장 가능
- row 단위 강한 일관성 (한 Row에 대한 읽기/쓰기는 단일 RegionServer에서 처리)

이런 특성 때문에 HBase는 다음과 같은 데이터에 적합하다.

- 매우 큰 규모(수십억 row)의 시계열, 로그, 이벤트 데이터
- PK 기반 랜덤 읽기/쓰기가 많은 워크로드
- 버전 관리가 필요한 이력/히스토리 데이터


### 5.9.2 HBase의 특징 정리

- 스키마 유연성
  - Column Family만 미리 정의, Column Qualifier는 동적으로 추가 가능
- 강한 일관성
  - 특정 Row Key에 대한 쓰기/읽기는 단일 RegionServer가 담당
- 수평 확장
  - Region 단위로 자동 분할/재배치, 클러스터 노드 추가만으로 확장 가능
- HDFS 기반 내구성
  - 데이터 파일(HFile)과 WAL은 HDFS의 복제 메커니즘으로 내구성 확보
- 대기 시간이 짧은 랜덤 읽기
  - MemStore + BlockCache를 활용한 캐싱 구조


---

## 5.9.1 HBase 자세히 살펴보기

### 5.9.1.1 HBase 개념과 아키텍처

주요 구성 요소:

- HMaster
  - 메타데이터 관리 (테이블/컬럼 패밀리 스키마)
  - Region 할당/재할당
  - RegionServer 로드 밸런싱
  - Region 분할(split), 병합(merge) 관리
- RegionServer
  - 실제 읽기/쓰기 요청 처리
  - 여러 개의 Region을 호스팅
- ZooKeeper
  - 클러스터 메타 정보 공유 (현재 Master/RegionServer 목록, 상태)
  - 장애 감지, 리더 선출 등 조정 기능
- HDFS
  - HFile(실제 데이터 파일), WAL(Write-Ahead Log)을 저장하는 분산 파일 시스템

데이터 단위:

- 테이블 → 여러 Region으로 분할
- Region → 특정 Row Key 범위 담당
- Region 안에는 여러 Column Family의 Store가 존재
- Store는 여러 HFile과 MemStore로 구성


### 5.9.1.2 META 테이블과 .META 서버

HBase는 “어느 Row Key가 어느 RegionServer에 있는지”를 알아야 한다.

- hbase:meta 테이블
  - 과거 .META 테이블이라 부르던 메타 정보 테이블
  - 각 Region의 범위(start key, end key)와 해당 Region을 담당하는 RegionServer 위치 정보 저장
- 동작 방식
  - 클라이언트는 우선 ZooKeeper를 통해 hbase:meta 위치를 찾는다.
  - hbase:meta에서 특정 테이블/Row Key가 속한 Region과 RegionServer를 조회
  - 결과를 클라이언트 측에 캐싱하여 이후 요청은 바로 RegionServer로 보냄
  - Region 이동/분할 발생 시 캐시를 갱신


### 5.9.1.3 리전 서버 구성 요소

RegionServer 내부 주요 컴포넌트:

- Region
  - Row Key 범위 단위 데이터 집합
- Store
  - 하나의 Column Family에 해당
- MemStore
  - 메모리 상의 쓰기 버퍼
  - 새로운 데이터는 먼저 여기에 기록
- HFile
  - 실질적인 데이터 파일
  - 정렬된 SSTable 형태, HDFS에 저장
- BlockCache
  - HFile의 블록을 메모리에 캐시하는 컴포넌트
  - 자주 사용하는 데이터를 메모리에서 바로 읽기
- WAL(Write-Ahead Log)
  - RegionServer 단위 로그 파일
  - 장애 발생 시 MemStore에 있던 데이터를 복구하기 위해 사용


### 5.9.1.4 HBase의 작업 실행 과정 (클라이언트 관점)

1. 클라이언트가 테이블/Row Key에 대한 읽기 또는 쓰기 요청을 보냄
2. Row Key → 어느 Region에 속하는지 알아야 함
   - 처음에는 hbase:meta를 조회해 Region 위치를 알아냄
   - 결과를 클라이언트 캐시
3. 이후 요청은 해당 RegionServer에 직접 전송
4. RegionServer가 요청을 처리하고 결과를 반환

이 과정을 통해 Master는 데이터 I/O에 직접 관여하지 않고,  
RegionServer가 직접 클라이언트 I/O를 처리하여 확장성을 확보한다.


### 5.9.1.5 HBase의 쓰기 작업 과정

쓰기(write) 흐름:

1. 클라이언트가 RegionServer에 Put/Delete 요청
2. RegionServer는 먼저 WAL에 로그를 append
3. 동시에 MemStore에 데이터 반영
4. WAL 기록 + MemStore 반영이 완료되면 클라이언트에 성공 응답
5. MemStore가 특정 크기를 넘으면 Flush 트리거
   - MemStore 내용을 HFile로 내려 보냄 (HDFS에 저장)
6. 시간이 지나 여러 HFile이 쌓이면 Compaction이 발생
   - 작은 파일들을 병합하고, 삭제/만료된 데이터는 제거

이 구조 덕분에:
- 쓰기 시 디스크 random write를 최소화하고 append 패턴 유지
- 장애 시 WAL을 이용해 MemStore 내용을 복구


### 5.9.1.6 HBase 리전 플러시(Region Flush)

Region Flush란, MemStore의 내용을 디스크(HFile)로 내리는 작업이다.

- 언제 발생하는가?
  - MemStore 크기가 임계값을 초과할 때
  - WAL 파일이 일정 크기를 넘어갈 때
  - 관리자 명령 또는 주기적인 자동 플러시
- 과정
  - MemStore 데이터를 정렬된 형태로 HFile에 기록
  - HDFS에 저장 완료 후, Flush 시점 이전 WAL 항목은 더 이상 복구용으로 필요 없게 됨 (일부 truncate/roll 가능)
- 효과
  - 메모리 회수
  - HFile 수 증가 → 이후 Compaction을 통해 다시 줄임


### 5.9.1.7 HBase 읽기 작업 과정

읽기(read) 흐름:

1. 클라이언트 → RegionServer에 Get/Scan 요청
2. RegionServer는 먼저 BlockCache를 확인
   - 캐시에 있으면 바로 반환
3. 캐시에 없으면:
   - MemStore에서 해당 Row/Column 조회
   - MemStore에 없으면 HFile을 탐색
4. 여러 버전/삭제 마커 등은 읽기 시점에 머지되어 최종 값으로 반환
5. 자주 읽는 데이터는 BlockCache에 적재되어 이후 조회 속도 향상

이러한 구조 덕분에 HBase는:
- 자주 조회되는 Row/컬럼에 대해 낮은 지연시간을 제공
- 대량의 데이터도 디스크 스캔 + 캐시 조합으로 처리 가능

## 번외) HBase에서의 최종 일관성(Eventual Consistency) 보장 방식

HBase는 사용자 관점에서는  
**Row 단위 강한 일관성(Strong Consistency)**을 제공하는 것처럼 보이지만,  
시스템 내부적으로는 **최종 일관성 모델을 기반으로 동작**한다.

이는 HBase가 분산 시스템의 가용성과 확장성을 확보하기 위해  
일관성 수준을 계층적으로 선택했기 때문이다.

---

### 1. Row 단위 강한 일관성과 클러스터 관점의 일관성

HBase의 일관성은 두 단계로 나누어 이해해야 한다.

- 단일 Row Key에 대한 연산  
  - 하나의 RegionServer가 전담
  - 읽기/쓰기가 직렬화됨
  - Strong Consistency 보장

- 클러스터 전체 관점  
  - Region 이동, 장애 복구, 메타데이터 동기화 과정에서
  - 상태 변경은 점진적으로 전파됨
  - Eventual Consistency 특성 존재

즉, HBase는  
**Row 단위로는 강한 일관성**,  
**시스템 상태 관점에서는 최종 일관성**을 가진다.

---

### 2. Write-Ahead Log(WAL) 기반 쓰기 안정성

HBase의 쓰기는 다음 순서로 처리된다.

1. RegionServer는 먼저 WAL에 쓰기 로그를 기록
2. 이후 MemStore에 데이터를 반영
3. WAL 기록이 완료되면 클라이언트에 성공 응답 반환

WAL은 HDFS에 저장되며 복제되기 때문에  
단일 노드 장애가 발생하더라도 데이터를 유실하지 않는다.

이 방식은 즉각적인 복제 동기화 대신  
장애 복구 시 로그를 재적용하여 상태를 맞추는 구조로,  
최종 일관성 모델에 기반한 설계이다.

---

### 3. MemStore와 HFile 간 비동기적 플러시

HBase에서 쓰기 데이터는 즉시 디스크에 반영되지 않는다.

- 새로운 데이터는 MemStore에 먼저 저장
- 일정 조건에서 비동기적으로 HFile로 flush
- 여러 HFile은 이후 compaction을 통해 병합됨

이 과정 동안:

- 서로 다른 HFile 간 데이터 상태가 일시적으로 다를 수 있음
- 삭제 마커(Tombstone)와 기존 데이터가 공존할 수 있음

최종적으로 compaction이 완료되면  
데이터 상태가 정합된 형태로 수렴한다.

---

### 4. 리전 이동과 장애 복구에서의 최종 일관성

RegionServer 장애가 발생하면:

1. ZooKeeper가 장애를 감지
2. 해당 서버가 보유한 Region을 다른 RegionServer로 재할당
3. 새로운 RegionServer는 WAL을 replay하여 MemStore 상태를 복구
4. 서비스는 정상적으로 재개됨

이 과정에서:

- 클러스터 메타 상태는 점진적으로 전파됨
- 장애 직후 짧은 시간 동안 일부 요청이 실패하거나 지연될 수 있음
- 복구 이후에는 데이터가 일관된 상태로 수렴

이는 강한 즉시 일관성보다는  
**장애 허용성을 우선한 최종 일관성 모델의 전형적인 동작 방식**이다.

---

### 5. META 테이블과 메타데이터의 최종 일관성

hbase:meta 테이블은 다음 정보를 관리한다.

- Region 범위
- RegionServer 위치

Region 분할이나 이동 시:

- 메타 정보는 먼저 변경
- 클라이언트 캐시는 이후 갱신
- 잘못된 RegionServer로 요청이 가면 재조회 수행

이 과정에서 메타데이터는  
짧은 시간 동안 일관되지 않을 수 있으나,  
결과적으로 올바른 상태로 수렴한다.

---

### 6. HBase에서 최종 일관성을 선택한 이유

HBase가 최종 일관성 모델을 내재한 이유는 다음과 같다.

- 대규모 분산 환경에서의 가용성 확보
- 네트워크 파티션에 대한 내성
- 수평 확장성
- 장애 복구 시 전체 중단 방지

즉시 글로벌 일관성을 강제하는 대신,  
**Row 단위 강한 일관성 + 시스템 단위 최종 일관성**이라는 타협을 선택한 것이다.
